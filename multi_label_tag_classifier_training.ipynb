{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "multi_label_tag_classifier_training.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S72wsuKieCVJ",
        "colab_type": "text"
      },
      "source": [
        "## CNN classifier for multi label tagging of Myntra fashion products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDUdzgI23vrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "3095e29a-2075-45a0-9315-3430324f4426"
      },
      "source": [
        "# Execute only in colab\n",
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.15.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? n\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 5, in <module>\n",
            "    from pip._internal.main import main\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/main.py\", line 13, in <module>\n",
            "    from pip._internal.cli.autocompletion import autocomplete\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/autocompletion.py\", line 11, in <module>\n",
            "    from pip._internal.cli.main_parser import create_main_parser\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\n",
            "    from pip._internal.cli import cmdoptions\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/cli/cmdoptions.py\", line 24, in <module>\n",
            "    from pip._internal.exceptions import CommandError\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_internal/exceptions.py\", line 10, in <module>\n",
            "    from pip._vendor.six import iteritems\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pip/_vendor/six.py\", line 734, in <module>\n",
            "    \"\"\")\n",
            "  File \"<string>\", line 1, in <module>\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ReM7RBLVxHdi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "7b7124bb-7717-4bc4-bdfc-b735657b5e5c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import PIL\n",
        "import sys\n",
        "from matplotlib import pyplot\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import fbeta_score\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import backend\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJo65ZZHpr65",
        "colab_type": "text"
      },
      "source": [
        "##### If you are working on your local computer. Download and extract the dataset from https://www.kaggle.com/paramaggarwal/fashion-product-images-small\n",
        "##### As the productDisplayName column in styles.csv needed modification. Extract only the images folder into your CNN_model_fashion_products_multi_label_tagging/images/ directory. Make use of styles.csv from this repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "met2wttxxV18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder_path = os.getcwd()\n",
        "# folder_path = os.path.join(folder_path, 'CNN_model_fashion_products_multi_label_tagging')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anA0DGO7rIZO",
        "colab_type": "text"
      },
      "source": [
        "### Run below cell only in google colab\n",
        "##### Upload your kaggle.json file into colab before you run below cell.\n",
        "##### Download the kaggle.json file from your kaggle Profile/Account/API by clicking \"Create New API Token\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWWcK0MweCVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/sharathrjtr/CNN_model_fashion_products_multi_label_tagging.git\n",
        "folder_path = os.path.join(folder_path, 'CNN_model_fashion_products_multi_label_tagging')\n",
        "\n",
        "# Download the Kaggle Myntra fashion products small dataset\n",
        "# For details: https://github.com/Kaggle/kaggle-api\n",
        "\n",
        "# Two methods to authenticate your kaggle account. \n",
        "\n",
        "# Method 1: Fill in the details of username and key below from the kaggle.json.\n",
        "# os.environ['KAGGLE_USERNAME'] = \"username\"\n",
        "# os.environ['KAGGLE_KEY'] = \"xxxxxxxxxxxxxx\"\n",
        "\n",
        "# Method 2: If kaggle.json is already uploaded in colab directory /content/\n",
        "os.environ['KAGGLE_CONFIG_DIR']='/content/'\n",
        "\n",
        "!kaggle datasets download -d paramaggarwal/fashion-product-images-small\n",
        "\n",
        "# Unzip the download dataset into the CNN model directory excluding the styles.csv and directory myntradataset/\n",
        "!unzip fashion-product-images-small.zip -x styles.csv -x myntradataset/* -d CNN_model_fashion_products_multi_label_tagging/myntradataset/\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flJyg9rUeCVZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cleanup_csv_mapping(mapping_csv):\n",
        "    # articleType can be used to find subCategory and masterCategory. \n",
        "    # Year specifies particular year the product is made, productDisplayName is something unique to the product and not a generalization.\n",
        "    # Hence, we are dropping masterCategory, subCartegory, year and productDisplayName columns from the data frame.\n",
        "    mapping_csv.drop(['masterCategory', 'subCategory', 'year', 'productDisplayName'], axis=1, inplace=True)\n",
        "\n",
        "    # Drop the samples which don't have any value in any of the columns.\n",
        "    mapping_csv.dropna(inplace=True)\n",
        "    mapping_csv.head()\n",
        "    \n",
        "    return mapping_csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07CxqNIBeCVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a list of tags available among gender, articleType, baseColour, season and usage\n",
        "# generate a mapping from tags to integers and integers to tags\n",
        "def extract_tags_mapping(mapping_csv):\n",
        "    labels = set()\n",
        "    \n",
        "    for index, row in mapping_csv.iterrows():\n",
        "        fileid = row['id']\n",
        "        gender = row['gender']\n",
        "        article_type = row['articleType']\n",
        "        base_colour = row['baseColour']\n",
        "        season = row['season']\n",
        "        usage = row['usage']\n",
        "        \n",
        "        labels.update([gender, article_type, base_colour, season, usage])\n",
        "    print('Total labels:', len(labels))\n",
        "\n",
        "    # convert the labels to a list and sort them alphabetically\n",
        "    labels = list(labels)\n",
        "    # order set alphabetically\n",
        "    labels.sort()\n",
        "    \n",
        "    # create dictionary that maps labels to integers so that we can encode the training dataset for modeling.\n",
        "    # create a dictionary with reverse mapping from integers to string tag values, so later when the model makes a prediction, we can turn it into something readable.\n",
        "    labels_map = {labels[i]: i for i in range(len(labels))}\n",
        "    inv_labels_map = {i:labels[i] for i in range(len(labels))}\n",
        "    \n",
        "    return labels_map, inv_labels_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXGLDtn8eCVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extract the training images filename and labels for all images.\n",
        "def extract_img_ids_labels(mapping_csv):\n",
        "    image_ids = []\n",
        "    image_labels = dict()\n",
        "    for index, row in mapping_csv.iterrows():\n",
        "        fileid = row['id']\n",
        "        gender = row['gender']\n",
        "        article_type = row['articleType']\n",
        "        base_colour = row['baseColour']\n",
        "        season = row['season']\n",
        "        usage = row['usage']\n",
        "        \n",
        "        if os.path.exists(folder_path+'/myntradataset/images/'+str(fileid)+'.jpg'):\n",
        "            image_ids.append(fileid)\n",
        "            image_labels[fileid] = [gender, article_type, base_colour, season, usage]\n",
        "    \n",
        "    print('Number of train files: ', len(image_ids))\n",
        "    \n",
        "    return image_ids, image_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSVoikLheCVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a one hot encoding for one list of tags\n",
        "def one_hot_encode(tags, mapping):\n",
        "    # create empty vector\n",
        "    encoding = np.zeros(len(mapping), dtype='uint8')\n",
        "    # mark 1 for each tag in the vector\n",
        "    for tag in tags:\n",
        "        encoding[mapping[tag]] = 1\n",
        "    return encoding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgcbWDOleCVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load images and extract labels in one hot encode form\n",
        "def load_dataset(image_ids, image_labels, tag_mapping):\n",
        "    images, targets = list(), list()\n",
        "    # enumerate file in the directory\n",
        "    for filename in image_ids:\n",
        "        # load image\n",
        "        image = load_img(os.path.join(folder_path+'/myntradataset/images', str(filename)+'.jpg'), target_size=(60,80))\n",
        "        # convert to numpy array\n",
        "        image = img_to_array(image, dtype='uint8')\n",
        "        # get tags\n",
        "        tags = image_labels[filename]\n",
        "        # one hot encode tags\n",
        "        target = one_hot_encode(tags, tag_mapping)\n",
        "        # store train image and tags\n",
        "        images.append(image)\n",
        "        targets.append(target)\n",
        "    \n",
        "    X = np.asarray(images, dtype='uint8')\n",
        "    y = np.asarray(targets, dtype='uint8')\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfd6W0PpeCVv",
        "colab_type": "code",
        "outputId": "43d4b485-acfc-4f8d-86d8-f8fb1dd46318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "csv_filename = folder_path+'/myntradataset/styles.csv'\n",
        "# Read the csv file and clean up the file to extract relevant data\n",
        "# productDisplayName column consist of extra commas for few row. \n",
        "# These were manually removed as it can result in error during reading of the file\n",
        "mapping_csv = pd.read_csv(csv_filename)\n",
        "print('Dataset Mapping dataframe size before cleanup: ', mapping_csv.shape)\n",
        "\n",
        "mapping_csv = cleanup_csv_mapping(mapping_csv)\n",
        "print('Dataset Mapping dataframe size after cleanup: ', mapping_csv.shape)\n",
        "mapping_csv.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Mapping dataframe size before cleanup:  (44446, 10)\n",
            "Dataset Mapping dataframe size after cleanup:  (44101, 6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>articleType</th>\n",
              "      <th>baseColour</th>\n",
              "      <th>season</th>\n",
              "      <th>usage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>15970</td>\n",
              "      <td>Men</td>\n",
              "      <td>Shirts</td>\n",
              "      <td>Navy Blue</td>\n",
              "      <td>Fall</td>\n",
              "      <td>Casual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39386</td>\n",
              "      <td>Men</td>\n",
              "      <td>Jeans</td>\n",
              "      <td>Blue</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Casual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>59263</td>\n",
              "      <td>Women</td>\n",
              "      <td>Watches</td>\n",
              "      <td>Silver</td>\n",
              "      <td>Winter</td>\n",
              "      <td>Casual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>21379</td>\n",
              "      <td>Men</td>\n",
              "      <td>Track Pants</td>\n",
              "      <td>Black</td>\n",
              "      <td>Fall</td>\n",
              "      <td>Casual</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53759</td>\n",
              "      <td>Men</td>\n",
              "      <td>Tshirts</td>\n",
              "      <td>Grey</td>\n",
              "      <td>Summer</td>\n",
              "      <td>Casual</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id gender  articleType baseColour  season   usage\n",
              "0  15970    Men       Shirts  Navy Blue    Fall  Casual\n",
              "1  39386    Men        Jeans       Blue  Summer  Casual\n",
              "2  59263  Women      Watches     Silver  Winter  Casual\n",
              "3  21379    Men  Track Pants      Black    Fall  Casual\n",
              "4  53759    Men      Tshirts       Grey  Summer  Casual"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwqL__UseCV0",
        "colab_type": "code",
        "outputId": "6e45974d-5e0d-4ab8-ae03-3cd86a6dd374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# extract mapping from labels to indices and indices to labels\n",
        "labels_to_idx, idx_to_labels = extract_tags_mapping(mapping_csv)\n",
        "\n",
        "# extract the images filename and tags for training dataset\n",
        "train_ids, train_labels = extract_img_ids_labels(mapping_csv)\n",
        "\n",
        "# load the dataset, the actual images and labels for each image with one-hot encoding\n",
        "train_images, train_tags = load_dataset(train_ids, train_labels, labels_to_idx)\n",
        "print('Total Images shape: ', train_images.shape, 'Total tags shape: ', train_tags.shape)\n",
        "\n",
        "# Total size of image loaded will be about 60*80*3*44101*8 / (1000000000*8) = 0.635GB\n",
        "# save both arrays to one file in compressed format\n",
        "np.savez_compressed(folder_path+'/myntra_train_data.npz', train_images, train_tags)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total labels: 205\n",
            "Number of train files:  44096\n",
            "Total Images shape:  (44096, 60, 80, 3) Total tags shape:  (44096, 205)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "velcC-GOxrH8",
        "colab_type": "text"
      },
      "source": [
        "#### From here on we work on the saved myntra_train_data.npz file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJmlHPJBeCV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_saved_dataset():\n",
        "    data = np.load(folder_path+'/myntra_train_data.npz')\n",
        "    X, y = data['arr_0'], data['arr_1']\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
        "    print('Train images Shape: ', trainX.shape, 'Train labels shape:', trainY.shape)\n",
        "    print('Test images shape: ', testX.shape, 'Test labels shape: ', testY.shape)\n",
        "    \n",
        "    return trainX, trainY, testX, testY"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_VD6DjKeCWB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # calculate fbeta score for multi-class/label classification\n",
        "def fbeta(y_true, y_pred, beta=2):\n",
        "    # clip predictions\n",
        "    y_pred = keras.backend.clip(y_pred, 0, 1)\n",
        "    # calculate elements\n",
        "    tp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true * y_pred, 0, 1)), axis=1)\n",
        "    fp = keras.backend.sum(keras.backend.round(keras.backend.clip(y_pred - y_true, 0, 1)), axis=1)\n",
        "    fn = keras.backend.sum(keras.backend.round(keras.backend.clip(y_true - y_pred, 0, 1)), axis=1)\n",
        "    # calculate precision\n",
        "    p = tp / (tp + fp + keras.backend.epsilon())\n",
        "    # calculate recall\n",
        "    r = tp / (tp + fn + keras.backend.epsilon())\n",
        "    # calculate fbeta, averaged across each class\n",
        "    bb = beta ** 2\n",
        "    fbeta_score = keras.backend.mean((1 + bb) * (p * r) / (bb * p + r + keras.backend.epsilon()))\n",
        "    return fbeta_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R36S3gfPeCWF",
        "colab_type": "code",
        "outputId": "ecc4438e-1889-48c8-daea-3df4fba23fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "trainX, trainY, testX, testY = load_saved_dataset()\n",
        "\n",
        "# make all one predictions\n",
        "train_yhat = np.asarray([np.ones(trainY.shape[1]) for _ in range(trainY.shape[0])])\n",
        "test_yhat = np.asarray([np.ones(testY.shape[1]) for _ in range(testY.shape[0])])\n",
        "# evaluate predictions\n",
        "train_score = fbeta_score(trainY, train_yhat, 2, average='samples')\n",
        "test_score = fbeta_score(testY, test_yhat, 2, average='samples')\n",
        "print('All Ones: train=%.3f, test=%.3f' % (train_score, test_score))\n",
        "\n",
        "# evaluate predictions with keras\n",
        "train_score = fbeta(keras.backend.variable(trainY), keras.backend.variable(train_yhat))\n",
        "test_score = fbeta(keras.backend.variable(testY), keras.backend.variable(test_yhat))\n",
        "\n",
        "if tf.__version__ > \"1.15.0\":\n",
        "  print('All Ones (keras): train=%.3f, test=%.3f' % (train_score, test_score))\n",
        "else:\n",
        "  print('All Ones (keras): train=%.3f, test=%.3f' % (keras.backend.eval(train_score), keras.backend.eval(test_score)))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images Shape:  (30867, 60, 80, 3) Train labels shape: (30867, 205)\n",
            "Test images shape:  (13229, 60, 80, 3) Test labels shape:  (13229, 205)\n",
            "All Ones: train=0.111, test=0.111\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "All Ones (keras): train=0.111, test=0.111\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBmV_pfQeCWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cnn model\n",
        "def define_baseline_model(in_shape=(60,80, 3), out_shape=205):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dense(out_shape, activation='sigmoid'))\n",
        "    # compile model\n",
        "    opt = SGD(lr=0.01, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaQXLQxdeCWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot diagnostic learning curves\n",
        "def summarize_diagnostics(history, figname, epochs):\n",
        "    fig = pyplot.figure(figsize=(15,5))\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # plot loss\n",
        "    pyplot.subplot(121)    \n",
        "    pyplot.plot(range(1,epochs+1,1), history.history['loss'], color='blue', label='train_loss')\n",
        "    pyplot.plot(range(1,epochs+1,1), history.history['val_loss'], color='orange', label='val_loss')\n",
        "    pyplot.xlabel('Epochs')\n",
        "    pyplot.ylabel('Cross Entropy Loss')\n",
        "    pyplot.title(figname+': Cross Entropy Loss')\n",
        "    pyplot.legend()\n",
        "\n",
        "    # plot fbeta score\n",
        "    pyplot.subplot(122)\n",
        "    pyplot.plot(range(1,epochs+1,1), history.history['fbeta'], color='blue', label='train_fbeta_score')\n",
        "    pyplot.plot(range(1,epochs+1,1), history.history['val_fbeta'], color='orange', label='val_fbeta_score')\n",
        "    pyplot.xlabel('Epochs')\n",
        "    pyplot.ylabel('Fbeta Score')\n",
        "    pyplot.title(figname+': Fbeta Score')\n",
        "    pyplot.legend()\n",
        "\n",
        "    # save plot to file\n",
        "    filename = folder_path + '/' + figname\n",
        "    pyplot.savefig(filename + '_plot.png')\n",
        "    pyplot.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p8-3IcLeCWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run the test harness for evaluating a model\n",
        "def run_test_harness(model, figname, epochs, height_shift_range=0.0, width_shift_range=0.0, shear_range=0.0, hor_flip=False, vert_flip=False, rot_range=0, featurewise_center=False):\n",
        "    # load dataset\n",
        "    trainX, trainY, testX, testY = load_saved_dataset()\n",
        "    # create data generator\n",
        "    if featurewise_center:\n",
        "        datagen = ImageDataGenerator(featurewise_center = True, height_shift_range=height_shift_range, width_shift_range=width_shift_range, shear_range=shear_range, horizontal_flip=hor_flip, vertical_flip=vert_flip, rotation_range=rot_range)\n",
        "        # specify imagenet mean values for centering\n",
        "        datagen.mean = [123.68, 116.779, 103.939]\n",
        "    else:\n",
        "        datagen = ImageDataGenerator(rescale=1.0/255.0, height_shift_range=height_shift_range, width_shift_range=width_shift_range, shear_range=shear_range, horizontal_flip=hor_flip, vertical_flip=vert_flip, rotation_range=rot_range)\n",
        "    # prepare iterators\n",
        "    train_it = datagen.flow(trainX, trainY, batch_size=128)\n",
        "    test_it = datagen.flow(testX, testY, batch_size=128)\n",
        "    \n",
        "    # fit model\n",
        "    history = model.fit_generator(train_it, steps_per_epoch=len(train_it),\n",
        "        validation_data=test_it, validation_steps=len(test_it), epochs=epochs, verbose=1)\n",
        "    # evaluate model\n",
        "    loss, fbeta = model.evaluate_generator(test_it, steps=len(test_it), verbose=1)\n",
        "    print('> loss=%.3f, fbeta=%.3f' % (loss, fbeta))\n",
        "    # learning curves\n",
        "    summarize_diagnostics(history, figname, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hq0ZxmaZeCWU",
        "colab_type": "code",
        "outputId": "a79db66c-c9c8-4812-d650-64fdf2f6d761",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# run the test harness for baseline model\n",
        "model = define_baseline_model()\n",
        "run_test_harness(model, 'Baseline', 50)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images Shape:  (30867, 60, 80, 3) Train labels shape: (30867, 205)\n",
            "Test images shape:  (13229, 60, 80, 3) Test labels shape:  (13229, 205)\n",
            "Epoch 1/50\n",
            "242/242 [==============================] - 20s 84ms/step - loss: 0.1201 - fbeta: 0.2801 - val_loss: 0.0654 - val_fbeta: 0.2700\n",
            "Epoch 2/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0640 - fbeta: 0.3233 - val_loss: 0.0630 - val_fbeta: 0.3215\n",
            "Epoch 3/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0611 - fbeta: 0.3589 - val_loss: 0.0601 - val_fbeta: 0.3815\n",
            "Epoch 4/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0589 - fbeta: 0.3860 - val_loss: 0.0580 - val_fbeta: 0.4307\n",
            "Epoch 5/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0567 - fbeta: 0.4084 - val_loss: 0.0559 - val_fbeta: 0.3898\n",
            "Epoch 6/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0543 - fbeta: 0.4355 - val_loss: 0.0538 - val_fbeta: 0.4326\n",
            "Epoch 7/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0521 - fbeta: 0.4578 - val_loss: 0.0514 - val_fbeta: 0.4807\n",
            "Epoch 8/50\n",
            "242/242 [==============================] - 19s 81ms/step - loss: 0.0502 - fbeta: 0.4801 - val_loss: 0.0499 - val_fbeta: 0.4764\n",
            "Epoch 9/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0485 - fbeta: 0.5026 - val_loss: 0.0481 - val_fbeta: 0.4975\n",
            "Epoch 10/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0472 - fbeta: 0.5197 - val_loss: 0.0470 - val_fbeta: 0.5238\n",
            "Epoch 11/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0459 - fbeta: 0.5359 - val_loss: 0.0460 - val_fbeta: 0.5462\n",
            "Epoch 12/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0448 - fbeta: 0.5502 - val_loss: 0.0449 - val_fbeta: 0.5644\n",
            "Epoch 13/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0439 - fbeta: 0.5610 - val_loss: 0.0442 - val_fbeta: 0.5596\n",
            "Epoch 14/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0430 - fbeta: 0.5716 - val_loss: 0.0440 - val_fbeta: 0.5784\n",
            "Epoch 15/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0423 - fbeta: 0.5812 - val_loss: 0.0426 - val_fbeta: 0.5884\n",
            "Epoch 16/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0416 - fbeta: 0.5885 - val_loss: 0.0419 - val_fbeta: 0.5852\n",
            "Epoch 17/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0410 - fbeta: 0.5950 - val_loss: 0.0414 - val_fbeta: 0.5992\n",
            "Epoch 18/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0404 - fbeta: 0.6021 - val_loss: 0.0410 - val_fbeta: 0.6163\n",
            "Epoch 19/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0398 - fbeta: 0.6087 - val_loss: 0.0410 - val_fbeta: 0.5800\n",
            "Epoch 20/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0394 - fbeta: 0.6130 - val_loss: 0.0401 - val_fbeta: 0.6023\n",
            "Epoch 21/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0389 - fbeta: 0.6188 - val_loss: 0.0396 - val_fbeta: 0.6101\n",
            "Epoch 22/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0385 - fbeta: 0.6229 - val_loss: 0.0396 - val_fbeta: 0.6113\n",
            "Epoch 23/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0380 - fbeta: 0.6281 - val_loss: 0.0391 - val_fbeta: 0.6243\n",
            "Epoch 24/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0377 - fbeta: 0.6315 - val_loss: 0.0386 - val_fbeta: 0.6278\n",
            "Epoch 25/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0372 - fbeta: 0.6362 - val_loss: 0.0382 - val_fbeta: 0.6284\n",
            "Epoch 26/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0369 - fbeta: 0.6406 - val_loss: 0.0380 - val_fbeta: 0.6365\n",
            "Epoch 27/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0366 - fbeta: 0.6441 - val_loss: 0.0378 - val_fbeta: 0.6391\n",
            "Epoch 28/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0362 - fbeta: 0.6483 - val_loss: 0.0376 - val_fbeta: 0.6389\n",
            "Epoch 29/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0360 - fbeta: 0.6493 - val_loss: 0.0375 - val_fbeta: 0.6373\n",
            "Epoch 30/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0356 - fbeta: 0.6549 - val_loss: 0.0371 - val_fbeta: 0.6602\n",
            "Epoch 31/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0353 - fbeta: 0.6573 - val_loss: 0.0369 - val_fbeta: 0.6294\n",
            "Epoch 32/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0350 - fbeta: 0.6601 - val_loss: 0.0368 - val_fbeta: 0.6480\n",
            "Epoch 33/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0348 - fbeta: 0.6611 - val_loss: 0.0366 - val_fbeta: 0.6530\n",
            "Epoch 34/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0345 - fbeta: 0.6646 - val_loss: 0.0365 - val_fbeta: 0.6505\n",
            "Epoch 35/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0342 - fbeta: 0.6674 - val_loss: 0.0359 - val_fbeta: 0.6681\n",
            "Epoch 36/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0340 - fbeta: 0.6712 - val_loss: 0.0356 - val_fbeta: 0.6483\n",
            "Epoch 37/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0337 - fbeta: 0.6730 - val_loss: 0.0356 - val_fbeta: 0.6656\n",
            "Epoch 38/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0336 - fbeta: 0.6745 - val_loss: 0.0354 - val_fbeta: 0.6619\n",
            "Epoch 39/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0333 - fbeta: 0.6774 - val_loss: 0.0355 - val_fbeta: 0.6751\n",
            "Epoch 40/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0331 - fbeta: 0.6808 - val_loss: 0.0350 - val_fbeta: 0.6594\n",
            "Epoch 41/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0328 - fbeta: 0.6828 - val_loss: 0.0349 - val_fbeta: 0.6588\n",
            "Epoch 42/50\n",
            "242/242 [==============================] - 20s 81ms/step - loss: 0.0327 - fbeta: 0.6837 - val_loss: 0.0346 - val_fbeta: 0.6670\n",
            "Epoch 43/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0324 - fbeta: 0.6867 - val_loss: 0.0347 - val_fbeta: 0.6705\n",
            "Epoch 44/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0323 - fbeta: 0.6876 - val_loss: 0.0344 - val_fbeta: 0.6812\n",
            "Epoch 45/50\n",
            "242/242 [==============================] - 20s 81ms/step - loss: 0.0320 - fbeta: 0.6909 - val_loss: 0.0342 - val_fbeta: 0.6782\n",
            "Epoch 46/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0318 - fbeta: 0.6926 - val_loss: 0.0340 - val_fbeta: 0.6811\n",
            "Epoch 47/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0317 - fbeta: 0.6948 - val_loss: 0.0341 - val_fbeta: 0.6851\n",
            "Epoch 48/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0314 - fbeta: 0.6959 - val_loss: 0.0339 - val_fbeta: 0.6758\n",
            "Epoch 49/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0313 - fbeta: 0.6990 - val_loss: 0.0344 - val_fbeta: 0.6729\n",
            "Epoch 50/50\n",
            "242/242 [==============================] - 19s 80ms/step - loss: 0.0311 - fbeta: 0.7004 - val_loss: 0.0337 - val_fbeta: 0.6619\n",
            "104/104 [==============================] - 3s 25ms/step\n",
            "> loss=0.034, fbeta=0.662\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8pr6OlseCWZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cnn model with dropout\n",
        "def define_bl_dropout_model(in_shape=(60, 80, 3), out_shape=205):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=in_shape))\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(rate=0.2))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(rate=0.2))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Dropout(rate=0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "    model.add(Dropout(rate=0.5))\n",
        "    model.add(Dense(out_shape, activation='sigmoid'))\n",
        "    # compile model\n",
        "    opt = SGD(lr=0.01, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8uhB-SzeCWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58103de8-32e1-4ef8-8aae-2edf131a4f14"
      },
      "source": [
        "# entry point, run the test harness for dropout model without augmentation\n",
        "model = define_bl_dropout_model()\n",
        "run_test_harness(model, 'Bl_Dropout', 150)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train images Shape:  (30867, 60, 80, 3) Train labels shape: (30867, 205)\n",
            "Test images shape:  (13229, 60, 80, 3) Test labels shape:  (13229, 205)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/150\n",
            "242/242 [==============================] - 28s 116ms/step - loss: 0.2225 - fbeta: 0.2528 - val_loss: 0.0792 - val_fbeta: 0.2816\n",
            "Epoch 2/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0915 - fbeta: 0.3046 - val_loss: 0.0756 - val_fbeta: 0.3371\n",
            "Epoch 3/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0827 - fbeta: 0.3042 - val_loss: 0.0735 - val_fbeta: 0.2809\n",
            "Epoch 4/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0791 - fbeta: 0.3040 - val_loss: 0.0717 - val_fbeta: 0.2892\n",
            "Epoch 5/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0764 - fbeta: 0.3039 - val_loss: 0.0715 - val_fbeta: 0.2826\n",
            "Epoch 6/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0748 - fbeta: 0.3032 - val_loss: 0.0705 - val_fbeta: 0.3118\n",
            "Epoch 7/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0735 - fbeta: 0.3063 - val_loss: 0.0693 - val_fbeta: 0.2932\n",
            "Epoch 8/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0724 - fbeta: 0.3102 - val_loss: 0.0685 - val_fbeta: 0.2852\n",
            "Epoch 9/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0713 - fbeta: 0.3132 - val_loss: 0.0695 - val_fbeta: 0.2852\n",
            "Epoch 10/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0703 - fbeta: 0.3178 - val_loss: 0.0673 - val_fbeta: 0.3331\n",
            "Epoch 11/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0696 - fbeta: 0.3269 - val_loss: 0.0680 - val_fbeta: 0.3473\n",
            "Epoch 12/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0686 - fbeta: 0.3320 - val_loss: 0.0659 - val_fbeta: 0.3471\n",
            "Epoch 13/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0680 - fbeta: 0.3399 - val_loss: 0.0645 - val_fbeta: 0.3426\n",
            "Epoch 14/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0672 - fbeta: 0.3466 - val_loss: 0.0638 - val_fbeta: 0.3524\n",
            "Epoch 15/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0665 - fbeta: 0.3520 - val_loss: 0.0633 - val_fbeta: 0.3604\n",
            "Epoch 16/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0659 - fbeta: 0.3591 - val_loss: 0.0625 - val_fbeta: 0.3681\n",
            "Epoch 17/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0652 - fbeta: 0.3631 - val_loss: 0.0611 - val_fbeta: 0.3734\n",
            "Epoch 18/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0647 - fbeta: 0.3684 - val_loss: 0.0604 - val_fbeta: 0.3743\n",
            "Epoch 19/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0640 - fbeta: 0.3742 - val_loss: 0.0606 - val_fbeta: 0.3831\n",
            "Epoch 20/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0635 - fbeta: 0.3772 - val_loss: 0.0595 - val_fbeta: 0.3861\n",
            "Epoch 21/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0630 - fbeta: 0.3843 - val_loss: 0.0586 - val_fbeta: 0.3935\n",
            "Epoch 22/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0624 - fbeta: 0.3877 - val_loss: 0.0595 - val_fbeta: 0.3938\n",
            "Epoch 23/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0620 - fbeta: 0.3919 - val_loss: 0.0579 - val_fbeta: 0.4039\n",
            "Epoch 24/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0615 - fbeta: 0.3972 - val_loss: 0.0572 - val_fbeta: 0.4067\n",
            "Epoch 25/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0610 - fbeta: 0.3985 - val_loss: 0.0568 - val_fbeta: 0.4187\n",
            "Epoch 26/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0606 - fbeta: 0.4056 - val_loss: 0.0560 - val_fbeta: 0.4168\n",
            "Epoch 27/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0602 - fbeta: 0.4074 - val_loss: 0.0550 - val_fbeta: 0.4177\n",
            "Epoch 28/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0598 - fbeta: 0.4112 - val_loss: 0.0552 - val_fbeta: 0.4395\n",
            "Epoch 29/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0594 - fbeta: 0.4154 - val_loss: 0.0544 - val_fbeta: 0.4375\n",
            "Epoch 30/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0590 - fbeta: 0.4207 - val_loss: 0.0539 - val_fbeta: 0.4437\n",
            "Epoch 31/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0587 - fbeta: 0.4233 - val_loss: 0.0529 - val_fbeta: 0.4431\n",
            "Epoch 32/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0582 - fbeta: 0.4264 - val_loss: 0.0531 - val_fbeta: 0.4546\n",
            "Epoch 33/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0579 - fbeta: 0.4305 - val_loss: 0.0522 - val_fbeta: 0.4559\n",
            "Epoch 34/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0576 - fbeta: 0.4349 - val_loss: 0.0518 - val_fbeta: 0.4643\n",
            "Epoch 35/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0573 - fbeta: 0.4375 - val_loss: 0.0515 - val_fbeta: 0.4750\n",
            "Epoch 36/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0569 - fbeta: 0.4430 - val_loss: 0.0512 - val_fbeta: 0.4678\n",
            "Epoch 37/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0566 - fbeta: 0.4426 - val_loss: 0.0513 - val_fbeta: 0.4743\n",
            "Epoch 38/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0564 - fbeta: 0.4479 - val_loss: 0.0506 - val_fbeta: 0.4803\n",
            "Epoch 39/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0560 - fbeta: 0.4519 - val_loss: 0.0502 - val_fbeta: 0.4829\n",
            "Epoch 40/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0557 - fbeta: 0.4534 - val_loss: 0.0496 - val_fbeta: 0.4895\n",
            "Epoch 41/150\n",
            "242/242 [==============================] - 21s 85ms/step - loss: 0.0554 - fbeta: 0.4566 - val_loss: 0.0493 - val_fbeta: 0.4939\n",
            "Epoch 42/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0551 - fbeta: 0.4615 - val_loss: 0.0490 - val_fbeta: 0.4945\n",
            "Epoch 43/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0550 - fbeta: 0.4606 - val_loss: 0.0490 - val_fbeta: 0.5056\n",
            "Epoch 44/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0546 - fbeta: 0.4644 - val_loss: 0.0482 - val_fbeta: 0.5027\n",
            "Epoch 45/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0543 - fbeta: 0.4700 - val_loss: 0.0483 - val_fbeta: 0.5128\n",
            "Epoch 46/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0542 - fbeta: 0.4697 - val_loss: 0.0477 - val_fbeta: 0.5125\n",
            "Epoch 47/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0539 - fbeta: 0.4731 - val_loss: 0.0474 - val_fbeta: 0.5158\n",
            "Epoch 48/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0536 - fbeta: 0.4744 - val_loss: 0.0473 - val_fbeta: 0.5206\n",
            "Epoch 49/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0533 - fbeta: 0.4777 - val_loss: 0.0470 - val_fbeta: 0.5256\n",
            "Epoch 50/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0530 - fbeta: 0.4830 - val_loss: 0.0466 - val_fbeta: 0.5230\n",
            "Epoch 51/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0528 - fbeta: 0.4841 - val_loss: 0.0462 - val_fbeta: 0.5179\n",
            "Epoch 52/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0526 - fbeta: 0.4841 - val_loss: 0.0458 - val_fbeta: 0.5276\n",
            "Epoch 53/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0523 - fbeta: 0.4887 - val_loss: 0.0455 - val_fbeta: 0.5275\n",
            "Epoch 54/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0521 - fbeta: 0.4900 - val_loss: 0.0455 - val_fbeta: 0.5344\n",
            "Epoch 55/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0519 - fbeta: 0.4937 - val_loss: 0.0452 - val_fbeta: 0.5312\n",
            "Epoch 56/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0519 - fbeta: 0.4954 - val_loss: 0.0452 - val_fbeta: 0.5423\n",
            "Epoch 57/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0516 - fbeta: 0.4968 - val_loss: 0.0446 - val_fbeta: 0.5388\n",
            "Epoch 58/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0515 - fbeta: 0.4992 - val_loss: 0.0447 - val_fbeta: 0.5437\n",
            "Epoch 59/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0511 - fbeta: 0.5007 - val_loss: 0.0446 - val_fbeta: 0.5465\n",
            "Epoch 60/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0511 - fbeta: 0.5027 - val_loss: 0.0441 - val_fbeta: 0.5449\n",
            "Epoch 61/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0508 - fbeta: 0.5035 - val_loss: 0.0440 - val_fbeta: 0.5536\n",
            "Epoch 62/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0507 - fbeta: 0.5067 - val_loss: 0.0440 - val_fbeta: 0.5537\n",
            "Epoch 63/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0504 - fbeta: 0.5068 - val_loss: 0.0436 - val_fbeta: 0.5508\n",
            "Epoch 64/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0503 - fbeta: 0.5103 - val_loss: 0.0434 - val_fbeta: 0.5505\n",
            "Epoch 65/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0501 - fbeta: 0.5104 - val_loss: 0.0432 - val_fbeta: 0.5570\n",
            "Epoch 66/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0500 - fbeta: 0.5138 - val_loss: 0.0430 - val_fbeta: 0.5532\n",
            "Epoch 67/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0498 - fbeta: 0.5139 - val_loss: 0.0428 - val_fbeta: 0.5550\n",
            "Epoch 68/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0497 - fbeta: 0.5167 - val_loss: 0.0429 - val_fbeta: 0.5597\n",
            "Epoch 69/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0496 - fbeta: 0.5172 - val_loss: 0.0426 - val_fbeta: 0.5641\n",
            "Epoch 70/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0494 - fbeta: 0.5208 - val_loss: 0.0423 - val_fbeta: 0.5616\n",
            "Epoch 71/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0491 - fbeta: 0.5195 - val_loss: 0.0422 - val_fbeta: 0.5644\n",
            "Epoch 72/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0492 - fbeta: 0.5208 - val_loss: 0.0421 - val_fbeta: 0.5629\n",
            "Epoch 73/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0490 - fbeta: 0.5230 - val_loss: 0.0420 - val_fbeta: 0.5642\n",
            "Epoch 74/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0488 - fbeta: 0.5229 - val_loss: 0.0418 - val_fbeta: 0.5679\n",
            "Epoch 75/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0485 - fbeta: 0.5265 - val_loss: 0.0418 - val_fbeta: 0.5732\n",
            "Epoch 76/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0486 - fbeta: 0.5261 - val_loss: 0.0414 - val_fbeta: 0.5677\n",
            "Epoch 77/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0485 - fbeta: 0.5292 - val_loss: 0.0414 - val_fbeta: 0.5642\n",
            "Epoch 78/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0482 - fbeta: 0.5291 - val_loss: 0.0411 - val_fbeta: 0.5676\n",
            "Epoch 79/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0481 - fbeta: 0.5313 - val_loss: 0.0412 - val_fbeta: 0.5763\n",
            "Epoch 80/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0480 - fbeta: 0.5329 - val_loss: 0.0410 - val_fbeta: 0.5752\n",
            "Epoch 81/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0478 - fbeta: 0.5330 - val_loss: 0.0407 - val_fbeta: 0.5810\n",
            "Epoch 82/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0477 - fbeta: 0.5361 - val_loss: 0.0407 - val_fbeta: 0.5824\n",
            "Epoch 83/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0475 - fbeta: 0.5383 - val_loss: 0.0406 - val_fbeta: 0.5782\n",
            "Epoch 84/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0474 - fbeta: 0.5376 - val_loss: 0.0404 - val_fbeta: 0.5801\n",
            "Epoch 85/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0474 - fbeta: 0.5392 - val_loss: 0.0406 - val_fbeta: 0.5868\n",
            "Epoch 86/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0473 - fbeta: 0.5389 - val_loss: 0.0404 - val_fbeta: 0.5891\n",
            "Epoch 87/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0472 - fbeta: 0.5405 - val_loss: 0.0401 - val_fbeta: 0.5886\n",
            "Epoch 88/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0471 - fbeta: 0.5430 - val_loss: 0.0400 - val_fbeta: 0.5856\n",
            "Epoch 89/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0470 - fbeta: 0.5436 - val_loss: 0.0400 - val_fbeta: 0.5899\n",
            "Epoch 90/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0467 - fbeta: 0.5446 - val_loss: 0.0398 - val_fbeta: 0.5890\n",
            "Epoch 91/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0467 - fbeta: 0.5471 - val_loss: 0.0398 - val_fbeta: 0.5951\n",
            "Epoch 92/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0467 - fbeta: 0.5462 - val_loss: 0.0398 - val_fbeta: 0.5962\n",
            "Epoch 93/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0465 - fbeta: 0.5486 - val_loss: 0.0395 - val_fbeta: 0.5817\n",
            "Epoch 94/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0464 - fbeta: 0.5479 - val_loss: 0.0393 - val_fbeta: 0.5931\n",
            "Epoch 95/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0463 - fbeta: 0.5509 - val_loss: 0.0395 - val_fbeta: 0.5986\n",
            "Epoch 96/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0461 - fbeta: 0.5511 - val_loss: 0.0393 - val_fbeta: 0.5945\n",
            "Epoch 97/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0460 - fbeta: 0.5547 - val_loss: 0.0391 - val_fbeta: 0.5991\n",
            "Epoch 98/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0463 - fbeta: 0.5494 - val_loss: 0.0391 - val_fbeta: 0.6013\n",
            "Epoch 99/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0459 - fbeta: 0.5541 - val_loss: 0.0389 - val_fbeta: 0.6012\n",
            "Epoch 100/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0458 - fbeta: 0.5543 - val_loss: 0.0387 - val_fbeta: 0.6009\n",
            "Epoch 101/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0457 - fbeta: 0.5548 - val_loss: 0.0387 - val_fbeta: 0.5971\n",
            "Epoch 102/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0456 - fbeta: 0.5560 - val_loss: 0.0386 - val_fbeta: 0.5997\n",
            "Epoch 103/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0455 - fbeta: 0.5582 - val_loss: 0.0385 - val_fbeta: 0.6001\n",
            "Epoch 104/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0455 - fbeta: 0.5574 - val_loss: 0.0384 - val_fbeta: 0.6059\n",
            "Epoch 105/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0453 - fbeta: 0.5594 - val_loss: 0.0383 - val_fbeta: 0.5999\n",
            "Epoch 106/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0453 - fbeta: 0.5609 - val_loss: 0.0383 - val_fbeta: 0.6034\n",
            "Epoch 107/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0452 - fbeta: 0.5606 - val_loss: 0.0383 - val_fbeta: 0.6099\n",
            "Epoch 108/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0451 - fbeta: 0.5618 - val_loss: 0.0381 - val_fbeta: 0.6020\n",
            "Epoch 109/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0450 - fbeta: 0.5636 - val_loss: 0.0380 - val_fbeta: 0.6033\n",
            "Epoch 110/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0448 - fbeta: 0.5648 - val_loss: 0.0378 - val_fbeta: 0.6087\n",
            "Epoch 111/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0449 - fbeta: 0.5644 - val_loss: 0.0378 - val_fbeta: 0.6103\n",
            "Epoch 112/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0448 - fbeta: 0.5638 - val_loss: 0.0377 - val_fbeta: 0.6096\n",
            "Epoch 113/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0445 - fbeta: 0.5666 - val_loss: 0.0376 - val_fbeta: 0.6092\n",
            "Epoch 114/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0446 - fbeta: 0.5665 - val_loss: 0.0375 - val_fbeta: 0.6129\n",
            "Epoch 115/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0445 - fbeta: 0.5681 - val_loss: 0.0376 - val_fbeta: 0.6110\n",
            "Epoch 116/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0444 - fbeta: 0.5674 - val_loss: 0.0375 - val_fbeta: 0.6152\n",
            "Epoch 117/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0441 - fbeta: 0.5704 - val_loss: 0.0373 - val_fbeta: 0.6136\n",
            "Epoch 118/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0442 - fbeta: 0.5704 - val_loss: 0.0374 - val_fbeta: 0.6123\n",
            "Epoch 119/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0441 - fbeta: 0.5720 - val_loss: 0.0372 - val_fbeta: 0.6176\n",
            "Epoch 120/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0440 - fbeta: 0.5714 - val_loss: 0.0372 - val_fbeta: 0.6117\n",
            "Epoch 121/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0439 - fbeta: 0.5725 - val_loss: 0.0370 - val_fbeta: 0.6133\n",
            "Epoch 122/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0438 - fbeta: 0.5750 - val_loss: 0.0370 - val_fbeta: 0.6169\n",
            "Epoch 123/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0439 - fbeta: 0.5734 - val_loss: 0.0369 - val_fbeta: 0.6248\n",
            "Epoch 124/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0438 - fbeta: 0.5746 - val_loss: 0.0369 - val_fbeta: 0.6180\n",
            "Epoch 125/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0436 - fbeta: 0.5756 - val_loss: 0.0367 - val_fbeta: 0.6230\n",
            "Epoch 126/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0436 - fbeta: 0.5766 - val_loss: 0.0366 - val_fbeta: 0.6210\n",
            "Epoch 127/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0435 - fbeta: 0.5782 - val_loss: 0.0366 - val_fbeta: 0.6179\n",
            "Epoch 128/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0434 - fbeta: 0.5784 - val_loss: 0.0365 - val_fbeta: 0.6223\n",
            "Epoch 129/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0433 - fbeta: 0.5798 - val_loss: 0.0365 - val_fbeta: 0.6241\n",
            "Epoch 130/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0433 - fbeta: 0.5781 - val_loss: 0.0363 - val_fbeta: 0.6271\n",
            "Epoch 131/150\n",
            "242/242 [==============================] - 21s 86ms/step - loss: 0.0433 - fbeta: 0.5786 - val_loss: 0.0363 - val_fbeta: 0.6254\n",
            "Epoch 132/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0432 - fbeta: 0.5810 - val_loss: 0.0364 - val_fbeta: 0.6247\n",
            "Epoch 133/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0431 - fbeta: 0.5812 - val_loss: 0.0361 - val_fbeta: 0.6277\n",
            "Epoch 134/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0429 - fbeta: 0.5832 - val_loss: 0.0361 - val_fbeta: 0.6235\n",
            "Epoch 135/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0430 - fbeta: 0.5818 - val_loss: 0.0361 - val_fbeta: 0.6285\n",
            "Epoch 136/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0429 - fbeta: 0.5826 - val_loss: 0.0360 - val_fbeta: 0.6295\n",
            "Epoch 137/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0428 - fbeta: 0.5844 - val_loss: 0.0360 - val_fbeta: 0.6308\n",
            "Epoch 138/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0427 - fbeta: 0.5860 - val_loss: 0.0358 - val_fbeta: 0.6299\n",
            "Epoch 139/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0426 - fbeta: 0.5856 - val_loss: 0.0358 - val_fbeta: 0.6271\n",
            "Epoch 140/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0425 - fbeta: 0.5867 - val_loss: 0.0357 - val_fbeta: 0.6339\n",
            "Epoch 141/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0426 - fbeta: 0.5863 - val_loss: 0.0356 - val_fbeta: 0.6358\n",
            "Epoch 142/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0424 - fbeta: 0.5882 - val_loss: 0.0356 - val_fbeta: 0.6375\n",
            "Epoch 143/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0424 - fbeta: 0.5878 - val_loss: 0.0356 - val_fbeta: 0.6331\n",
            "Epoch 144/150\n",
            "242/242 [==============================] - 21s 88ms/step - loss: 0.0423 - fbeta: 0.5888 - val_loss: 0.0355 - val_fbeta: 0.6340\n",
            "Epoch 145/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0422 - fbeta: 0.5906 - val_loss: 0.0354 - val_fbeta: 0.6359\n",
            "Epoch 146/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0422 - fbeta: 0.5893 - val_loss: 0.0353 - val_fbeta: 0.6311\n",
            "Epoch 147/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0422 - fbeta: 0.5894 - val_loss: 0.0353 - val_fbeta: 0.6373\n",
            "Epoch 148/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0421 - fbeta: 0.5907 - val_loss: 0.0352 - val_fbeta: 0.6323\n",
            "Epoch 149/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0421 - fbeta: 0.5907 - val_loss: 0.0351 - val_fbeta: 0.6368\n",
            "Epoch 150/150\n",
            "242/242 [==============================] - 21s 87ms/step - loss: 0.0420 - fbeta: 0.5920 - val_loss: 0.0351 - val_fbeta: 0.6335\n",
            "104/104 [==============================] - 3s 25ms/step\n",
            "> loss=0.035, fbeta=0.634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz6J6eR9eCWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d6274d5-a717-4261-edba-949ddcc07b3b"
      },
      "source": [
        "# entry point, run the test harness for baseline model with augmentation\n",
        "model = define_baseline_model()\n",
        "run_test_harness(model, 'Bl_Img_Aug', 200, 0.1, 0.1, 10, True, True, 90)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images Shape:  (30867, 60, 80, 3) Train labels shape: (30867, 205)\n",
            "Test images shape:  (13229, 60, 80, 3) Test labels shape:  (13229, 205)\n",
            "Epoch 1/200\n",
            "242/242 [==============================] - 62s 258ms/step - loss: 0.1154 - fbeta: 0.2682 - val_loss: 0.0660 - val_fbeta: 0.2288\n",
            "Epoch 2/200\n",
            "242/242 [==============================] - 60s 248ms/step - loss: 0.0651 - fbeta: 0.2841 - val_loss: 0.0648 - val_fbeta: 0.2685\n",
            "Epoch 3/200\n",
            "242/242 [==============================] - 62s 255ms/step - loss: 0.0643 - fbeta: 0.2939 - val_loss: 0.0644 - val_fbeta: 0.2859\n",
            "Epoch 4/200\n",
            "242/242 [==============================] - 60s 249ms/step - loss: 0.0636 - fbeta: 0.3117 - val_loss: 0.0635 - val_fbeta: 0.3452\n",
            "Epoch 5/200\n",
            "242/242 [==============================] - 60s 248ms/step - loss: 0.0629 - fbeta: 0.3242 - val_loss: 0.0629 - val_fbeta: 0.3424\n",
            "Epoch 6/200\n",
            "242/242 [==============================] - 60s 248ms/step - loss: 0.0622 - fbeta: 0.3340 - val_loss: 0.0622 - val_fbeta: 0.3641\n",
            "Epoch 7/200\n",
            "242/242 [==============================] - 60s 246ms/step - loss: 0.0616 - fbeta: 0.3422 - val_loss: 0.0616 - val_fbeta: 0.3202\n",
            "Epoch 8/200\n",
            "242/242 [==============================] - 60s 248ms/step - loss: 0.0610 - fbeta: 0.3498 - val_loss: 0.0610 - val_fbeta: 0.3357\n",
            "Epoch 9/200\n",
            "242/242 [==============================] - 60s 248ms/step - loss: 0.0603 - fbeta: 0.3571 - val_loss: 0.0605 - val_fbeta: 0.3511\n",
            "Epoch 10/200\n",
            "242/242 [==============================] - 60s 249ms/step - loss: 0.0598 - fbeta: 0.3642 - val_loss: 0.0598 - val_fbeta: 0.3796\n",
            "Epoch 11/200\n",
            "242/242 [==============================] - 60s 249ms/step - loss: 0.0592 - fbeta: 0.3741 - val_loss: 0.0593 - val_fbeta: 0.3636\n",
            "Epoch 12/200\n",
            "242/242 [==============================] - 59s 246ms/step - loss: 0.0587 - fbeta: 0.3784 - val_loss: 0.0589 - val_fbeta: 0.3832\n",
            "Epoch 13/200\n",
            "242/242 [==============================] - 59s 244ms/step - loss: 0.0583 - fbeta: 0.3830 - val_loss: 0.0585 - val_fbeta: 0.3679\n",
            "Epoch 14/200\n",
            "242/242 [==============================] - 59s 243ms/step - loss: 0.0579 - fbeta: 0.3891 - val_loss: 0.0582 - val_fbeta: 0.4082\n",
            "Epoch 15/200\n",
            "242/242 [==============================] - 59s 245ms/step - loss: 0.0576 - fbeta: 0.3930 - val_loss: 0.0576 - val_fbeta: 0.4042\n",
            "Epoch 16/200\n",
            "242/242 [==============================] - 59s 245ms/step - loss: 0.0572 - fbeta: 0.3969 - val_loss: 0.0574 - val_fbeta: 0.3898\n",
            "Epoch 17/200\n",
            "242/242 [==============================] - 59s 245ms/step - loss: 0.0569 - fbeta: 0.3967 - val_loss: 0.0573 - val_fbeta: 0.3700\n",
            "Epoch 18/200\n",
            "242/242 [==============================] - 59s 243ms/step - loss: 0.0566 - fbeta: 0.4011 - val_loss: 0.0568 - val_fbeta: 0.3762\n",
            "Epoch 19/200\n",
            "242/242 [==============================] - 59s 243ms/step - loss: 0.0562 - fbeta: 0.4046 - val_loss: 0.0567 - val_fbeta: 0.4089\n",
            "Epoch 20/200\n",
            "242/242 [==============================] - 59s 242ms/step - loss: 0.0560 - fbeta: 0.4060 - val_loss: 0.0563 - val_fbeta: 0.3898\n",
            "Epoch 21/200\n",
            "242/242 [==============================] - 59s 243ms/step - loss: 0.0557 - fbeta: 0.4104 - val_loss: 0.0560 - val_fbeta: 0.4019\n",
            "Epoch 22/200\n",
            "242/242 [==============================] - 59s 246ms/step - loss: 0.0554 - fbeta: 0.4129 - val_loss: 0.0557 - val_fbeta: 0.4102\n",
            "Epoch 23/200\n",
            "242/242 [==============================] - 59s 243ms/step - loss: 0.0552 - fbeta: 0.4146 - val_loss: 0.0553 - val_fbeta: 0.4155\n",
            "Epoch 24/200\n",
            "242/242 [==============================] - 59s 246ms/step - loss: 0.0549 - fbeta: 0.4174 - val_loss: 0.0555 - val_fbeta: 0.4156\n",
            "Epoch 25/200\n",
            "242/242 [==============================] - 59s 245ms/step - loss: 0.0547 - fbeta: 0.4190 - val_loss: 0.0550 - val_fbeta: 0.4301\n",
            "Epoch 26/200\n",
            "242/242 [==============================] - 60s 246ms/step - loss: 0.0544 - fbeta: 0.4230 - val_loss: 0.0548 - val_fbeta: 0.4289\n",
            "Epoch 27/200\n",
            "242/242 [==============================] - 60s 246ms/step - loss: 0.0542 - fbeta: 0.4255 - val_loss: 0.0544 - val_fbeta: 0.4284\n",
            "Epoch 28/200\n",
            "242/242 [==============================] - 60s 246ms/step - loss: 0.0539 - fbeta: 0.4278 - val_loss: 0.0545 - val_fbeta: 0.4189\n",
            "Epoch 29/200\n",
            "242/242 [==============================] - 59s 245ms/step - loss: 0.0537 - fbeta: 0.4285 - val_loss: 0.0541 - val_fbeta: 0.4175\n",
            "Epoch 30/200\n",
            "242/242 [==============================] - 59s 244ms/step - loss: 0.0534 - fbeta: 0.4321 - val_loss: 0.0543 - val_fbeta: 0.4307\n",
            "Epoch 31/200\n",
            "242/242 [==============================] - 59s 244ms/step - loss: 0.0532 - fbeta: 0.4364 - val_loss: 0.0536 - val_fbeta: 0.4064\n",
            "Epoch 32/200\n",
            "242/242 [==============================] - 59s 243ms/step - loss: 0.0530 - fbeta: 0.4371 - val_loss: 0.0535 - val_fbeta: 0.4090\n",
            "Epoch 33/200\n",
            "242/242 [==============================] - 57s 237ms/step - loss: 0.0527 - fbeta: 0.4418 - val_loss: 0.0535 - val_fbeta: 0.4344\n",
            "Epoch 34/200\n",
            "242/242 [==============================] - 57s 236ms/step - loss: 0.0526 - fbeta: 0.4429 - val_loss: 0.0528 - val_fbeta: 0.4432\n",
            "Epoch 35/200\n",
            "242/242 [==============================] - 57s 235ms/step - loss: 0.0524 - fbeta: 0.4456 - val_loss: 0.0527 - val_fbeta: 0.4514\n",
            "Epoch 36/200\n",
            "242/242 [==============================] - 57s 234ms/step - loss: 0.0522 - fbeta: 0.4476 - val_loss: 0.0525 - val_fbeta: 0.4535\n",
            "Epoch 37/200\n",
            "242/242 [==============================] - 59s 246ms/step - loss: 0.0520 - fbeta: 0.4503 - val_loss: 0.0522 - val_fbeta: 0.4439\n",
            "Epoch 38/200\n",
            "242/242 [==============================] - 61s 253ms/step - loss: 0.0518 - fbeta: 0.4534 - val_loss: 0.0524 - val_fbeta: 0.4226\n",
            "Epoch 39/200\n",
            "242/242 [==============================] - 61s 251ms/step - loss: 0.0515 - fbeta: 0.4547 - val_loss: 0.0519 - val_fbeta: 0.4493\n",
            "Epoch 40/200\n",
            "242/242 [==============================] - 60s 250ms/step - loss: 0.0513 - fbeta: 0.4591 - val_loss: 0.0517 - val_fbeta: 0.4577\n",
            "Epoch 41/200\n",
            "242/242 [==============================] - 62s 256ms/step - loss: 0.0512 - fbeta: 0.4591 - val_loss: 0.0514 - val_fbeta: 0.4485\n",
            "Epoch 42/200\n",
            "242/242 [==============================] - 62s 258ms/step - loss: 0.0510 - fbeta: 0.4615 - val_loss: 0.0516 - val_fbeta: 0.4587\n",
            "Epoch 43/200\n",
            "242/242 [==============================] - 61s 251ms/step - loss: 0.0508 - fbeta: 0.4640 - val_loss: 0.0510 - val_fbeta: 0.4658\n",
            "Epoch 44/200\n",
            "242/242 [==============================] - 62s 257ms/step - loss: 0.0506 - fbeta: 0.4670 - val_loss: 0.0509 - val_fbeta: 0.4521\n",
            "Epoch 45/200\n",
            "242/242 [==============================] - 61s 251ms/step - loss: 0.0504 - fbeta: 0.4689 - val_loss: 0.0509 - val_fbeta: 0.4434\n",
            "Epoch 46/200\n",
            "242/242 [==============================] - 61s 254ms/step - loss: 0.0503 - fbeta: 0.4694 - val_loss: 0.0508 - val_fbeta: 0.4555\n",
            "Epoch 47/200\n",
            "242/242 [==============================] - 61s 252ms/step - loss: 0.0501 - fbeta: 0.4717 - val_loss: 0.0506 - val_fbeta: 0.4636\n",
            "Epoch 48/200\n",
            "242/242 [==============================] - 61s 251ms/step - loss: 0.0498 - fbeta: 0.4754 - val_loss: 0.0502 - val_fbeta: 0.4809\n",
            "Epoch 49/200\n",
            "242/242 [==============================] - 61s 251ms/step - loss: 0.0497 - fbeta: 0.4764 - val_loss: 0.0502 - val_fbeta: 0.4679\n",
            "Epoch 50/200\n",
            "242/242 [==============================] - 61s 252ms/step - loss: 0.0496 - fbeta: 0.4767 - val_loss: 0.0500 - val_fbeta: 0.4753\n",
            "Epoch 51/200\n",
            "242/242 [==============================] - 61s 253ms/step - loss: 0.0494 - fbeta: 0.4786 - val_loss: 0.0498 - val_fbeta: 0.4870\n",
            "Epoch 52/200\n",
            "242/242 [==============================] - 61s 252ms/step - loss: 0.0493 - fbeta: 0.4808 - val_loss: 0.0496 - val_fbeta: 0.4860\n",
            "Epoch 53/200\n",
            "  4/242 [..............................] - ETA: 18s - loss: 0.0488 - fbeta: 0.4931"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9ZLb1v0hKUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# files.download(folder_path+'/Baseline_plot.png')\n",
        "# files.download(folder_path+'/Bl_Dropout_plot.png')\n",
        "files.download(folder_path+'/Bl_Img_Aug_plot.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7r9NPU9eCWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define cnn model\n",
        "def define_vgg_model(in_shape=(60, 80, 3), out_shape=205, model_trainable=False):\n",
        "    # load model\n",
        "    model = VGG16(include_top=False, input_shape=in_shape)\n",
        "    # mark loaded layers as not trainable\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "    if model_trainable:\n",
        "        # allow last vgg block to be trainable\n",
        "        model.get_layer('block5_conv1').trainable = True\n",
        "        model.get_layer('block5_conv2').trainable = True\n",
        "        model.get_layer('block5_conv3').trainable = True\n",
        "        model.get_layer('block5_pool').trainable = True\n",
        "    # add new classifier layers\n",
        "    flat1 = Flatten()(model.layers[-1].output)\n",
        "    class1 = Dense(128, activation='relu', kernel_initializer='he_uniform')(flat1)\n",
        "    output = Dense(out_shape, activation='sigmoid')(class1)\n",
        "    # define new model\n",
        "    model = Model(inputs=model.inputs, outputs=output)\n",
        "    # compile model\n",
        "    opt = SGD(lr=0.01, momentum=0.9)\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[fbeta])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18UzNHVdeCWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# entry point, run the test harness for vgg model without augmentation\n",
        "model = define_vgg_model()\n",
        "run_test_harness(model, 'VGG', epochs=20, featurewise_center = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pQ5o4H5eCWp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# entry point, run the test harness for baseline model with augmentation\n",
        "model = define_vgg_model()\n",
        "run_test_harness(model, 'VGG_fine_tune', 50, featurewise_center = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uIduGO7eCWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# entry point, run the test harness for baseline model with augmentation\n",
        "model = define_vgg_model()\n",
        "run_test_harness(model, 'VGG_Img_Aug_fine_tune', 50, 0.1, 0.1, 10, True, True, 90, featurewise_center = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYOYrVfweCWy",
        "colab_type": "code",
        "outputId": "6da2a95f-c535-438d-8dd2-ad45e3c5fb06",
        "colab": {}
      },
      "source": [
        "l=np.linspace(0,1,11)\n",
        "# fig,ax=pyplot.figure(figsize=(6,4))\n",
        "pyplot.figure()\n",
        "pyplot.plot(range(11), l, label=['line'])\n",
        "pyplot.plot(range(11,0,-1), l, label=['line_rev'])\n",
        "pyplot.xlabel('Epochs')\n",
        "pyplot.legend()\n",
        "# pyplot.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fb2e6cb63c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}